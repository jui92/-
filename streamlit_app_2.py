import os
from typing import Dict, Any, List, Tuple

import numpy as np
import pandas as pd
from openai import OpenAI
import streamlit as st


# =========================================================
# 0. í™˜ê²½ ì„¤ì •
# =========================================================
OUTPUT_DIR = "output"  # GraphRAG ì¸ë±ì‹± ê²°ê³¼ í´ë”

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

st.set_page_config(page_title="Audit GraphRAG QA", layout="wide")


# =========================================================
# 1. ë°ì´í„° ë¡œë”© ìœ í‹¸
# =========================================================
def _detect_embedding_column(df: pd.DataFrame) -> str:
    """
    text_units.parquet ì—ì„œ ì„ë² ë”© ë²¡í„°ê°€ ë“¤ì–´ìˆëŠ” ì»¬ëŸ¼ëª…ì„ ìë™ íƒì§€.
    ê¸°ë³¸ì ìœ¼ë¡œ 'embedding' ì„ ê¸°ëŒ€í•˜ì§€ë§Œ, ì—†ìœ¼ë©´
    'vector', 'embeddings' ë“±ì˜ í›„ë³´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²€ì‚¬í•˜ê³ ,
    ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ íƒ€ì…(object) ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ.
    """
    candidate_cols = ["embedding", "embeddings", "vector", "vectors"]
    for col in candidate_cols:
        if col in df.columns:
            return col

    # fallback: ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì»¬ëŸ¼ ì°¾ê¸°
    for col in df.columns:
        if df[col].dtype == "object" and isinstance(df[col].dropna().iloc[0], (list, tuple)):
            return col

    raise ValueError("ì„ë² ë”© ë²¡í„° ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. text_units.parquet êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


@st.cache_resource
def load_graph_index(output_dir: str) -> Dict[str, Any]:
    """
    GraphRAG ì¸ë±ì‹± ê²°ê³¼(output ë””ë ‰í† ë¦¬)ë¥¼ ì½ì–´ì„œ
    ê²€ìƒ‰ì— í•„ìš”í•œ DataFrame/ë²¡í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦°ë‹¤.
    """
    data: Dict[str, Any] = {}

    # í•„ìˆ˜ íŒŒì¼ë“¤ ë¡œë”©
    text_units_path = os.path.join(output_dir, "text_units.parquet")
    documents_path = os.path.join(output_dir, "documents.parquet")
    entities_path = os.path.join(output_dir, "entities.parquet")
    relationships_path = os.path.join(output_dir, "relationships.parquet")
    communities_path = os.path.join(output_dir, "communities.parquet")

    if not os.path.exists(text_units_path):
        raise FileNotFoundError(f"{text_units_path} ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    if not os.path.exists(documents_path):
        raise FileNotFoundError(f"{documents_path} ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    text_units = pd.read_parquet(text_units_path)
    documents = pd.read_parquet(documents_path)
    entities = pd.read_parquet(entities_path) if os.path.exists(entities_path) else None
    relationships = (
        pd.read_parquet(relationships_path) if os.path.exists(relationships_path) else None
    )
    communities = (
        pd.read_parquet(communities_path) if os.path.exists(communities_path) else None
    )

    # ì„ë² ë”© ì»¬ëŸ¼ ìë™ íƒì§€ ë° ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜
    emb_col = _detect_embedding_column(text_units)
    embeddings_list = text_units[emb_col].tolist()
    embeddings = np.array(embeddings_list, dtype=np.float32)

    # case id / text / document id ë“± ì»¬ëŸ¼ëª… ìœ ì—° ì²˜ë¦¬
    id_col = "id" if "id" in text_units.columns else text_units.columns[0]
    text_col_candidates = ["text", "content", "body"]
    text_col = next((c for c in text_col_candidates if c in text_units.columns), None)
    if text_col is None:
        # fallback: ê°€ì¥ í…ìŠ¤íŠ¸ìŠ¤ëŸ¬ìš´ object ì»¬ëŸ¼
        obj_cols = [c for c in text_units.columns if text_units[c].dtype == "object"]
        text_col = obj_cols[0]

    # ë¬¸ì„œ human-readable id, title
    doc_id_col = "id" if "id" in documents.columns else documents.columns[0]
    doc_hr_id_col = (
        "human_readable_id" if "human_readable_id" in documents.columns else doc_id_col
    )
    doc_title_candidates = ["title", "name", "label", "doc_title"]
    doc_title_col = next((c for c in doc_title_candidates if c in documents.columns), None)

    data["text_units"] = text_units
    data["documents"] = documents
    data["entities"] = entities
    data["relationships"] = relationships
    data["communities"] = communities
    data["embeddings"] = embeddings
    data["id_col"] = id_col
    data["text_col"] = text_col
    data["doc_id_col"] = doc_id_col
    data["doc_hr_id_col"] = doc_hr_id_col
    data["doc_title_col"] = doc_title_col

    return data


# =========================================================
# 2. ê²€ìƒ‰ / ì¶”ì²œ ë¡œì§
# =========================================================
def get_embedding(text: str) -> np.ndarray:
    """OpenAI ì„ë² ë”© í˜¸ì¶œ."""
    if client is None:
        raise RuntimeError("OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    resp = client.embeddings.create(
        model="text-embedding-3-small",
        input=text,
    )
    return np.array(resp.data[0].embedding, dtype=np.float32)


def semantic_search(
    query: str,
    data: Dict[str, Any],
    top_k: int = 10,
) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    ì¿¼ë¦¬ë¥¼ ì„ë² ë”©í•˜ê³  text_units ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í›„,
    ìƒìœ„ top_k ê²°ê³¼ì™€ ì ìˆ˜ ë°˜í™˜.
    """
    q_vec = get_embedding(query)
    doc_vecs = data["embeddings"]  # (N, d)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    dot = np.dot(doc_vecs, q_vec)
    doc_norm = np.linalg.norm(doc_vecs, axis=1)
    q_norm = np.linalg.norm(q_vec) + 1e-8
    scores = dot / (doc_norm * q_norm + 1e-8)

    top_idx = np.argsort(scores)[-top_k:][::-1]
    top_scores = scores[top_idx]
    top_df = data["text_units"].iloc[top_idx].copy()
    top_df["similarity"] = top_scores

    return top_df, top_scores


def build_case_summary(
    row: pd.Series,
    data: Dict[str, Any],
) -> Dict[str, Any]:
    """
    text_unit í•œ ê±´ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œ/ì»¤ë®¤ë‹ˆí‹°/ì—”í‹°í‹° ì •ë³´ë¥¼ ëª¨ì•„
    LLMì—ê²Œ ë„˜ê¸°ê¸° ì¢‹ì€ êµ¬ì¡°ë¡œ ì •ë¦¬.
    """
    id_col = data["id_col"]
    text_col = data["text_col"]
    doc_id_col = data["doc_id_col"]
    doc_hr_id_col = data["doc_hr_id_col"]
    doc_title_col = data["doc_title_col"]

    text = row[text_col]
    # document_ids ì»¬ëŸ¼ì´ list í˜•íƒœë¡œ ìˆì„ ê°€ëŠ¥ì„±ì´ í¼
    doc_ids = []
    if "document_ids" in row:
        val = row["document_ids"]
        if isinstance(val, (list, tuple)):
            doc_ids = val
        elif pd.notna(val):
            doc_ids = [val]

    doc_meta_list = []
    if len(doc_ids) > 0:
        docs = data["documents"]
        for did in doc_ids:
            sub = docs[docs[doc_id_col] == did]
            if len(sub) == 0:
                continue
            d = sub.iloc[0]
            doc_meta_list.append(
                {
                    "id": d[doc_id_col],
                    "human_readable_id": d.get(doc_hr_id_col, d[doc_id_col]),
                    "title": d.get(doc_title_col, None),
                }
            )

    return {
        "text_id": row[id_col],
        "text": text,
        "documents": doc_meta_list,
    }


def llm_analyze_case(
    query: str,
    case_info: Dict[str, Any],
) -> str:
    """
    ë‹¨ì¼ ìœ ì‚¬ì‚¬ë¡€ì— ëŒ€í•´:
    - ìŸì  ìš”ì•½
    - ìœ ì‚¬ì /ì°¨ì´ì 
    - ê·¼ê±° ê¸°ë°˜ ì²˜ë¶„(ì£¼ì˜/ê²½ê³ /ì¤‘ì§•ê³„ ë“±) ì¶”ì²œ
    ì„ LLMìœ¼ë¡œ ìƒì„±.
    """
    if client is None:
        raise RuntimeError("OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    docs_str = ""
    for d in case_info["documents"]:
        docs_str += f"- ë¬¸ì„œID: {d.get('human_readable_id', d.get('id'))}, ì œëª©: {d.get('title')}\n"

    prompt = f"""
ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ë‚´ë¶€ê°ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆì˜]
{query}

[í›„ë³´ ìœ ì‚¬ì‚¬ë¡€ì˜ ë³¸ë¬¸]
{case_info['text']}

[í›„ë³´ ìœ ì‚¬ì‚¬ë¡€ì˜ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°]
{docs_str if docs_str else '(ë©”íƒ€ë°ì´í„° ì—†ìŒ)'}

ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ í˜•ì‹ì„ ê¼­ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

1. ì‚¬ê±´ ê°œìš” ìš”ì•½ (3~5ì¤„)
2. ìŸì  ë° ìœ„ë²•/ë¶€ë‹¹ ì†Œì§€
3. ê³¼ê±° ìœ ì‚¬ì‚¬ë¡€ì™€ì˜ ìœ ì‚¬ì /ì°¨ì´ì  (ì•Œ ìˆ˜ ìˆëŠ” ë²”ìœ„ì—ì„œ)
4. ê¶Œê³ ë˜ëŠ” ì²˜ë¶„ ìˆ˜ì¤€
   - ì˜ˆ: "ì£¼ì˜", "ê²½ê³ ", "ì£¼ì˜ ë° ì œë„ê°œì„  ê¶Œê³ ", "ì¤‘ì§•ê³„ ê²€í† " ë“±
5. ì²˜ë¶„ ìˆ˜ì¤€ì— ëŒ€í•œ ê·¼ê±° ì„¤ëª…
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” í•œêµ­ì˜ ê³µê³µê¸°ê´€ ë‚´ë¶€ê°ì‚¬ ì „ë¬¸ê°€ì´ë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content


def llm_recommend_overall(
    query: str,
    top_cases: List[Dict[str, Any]],
) -> str:
    """
    ìƒìœ„ ì—¬ëŸ¬ ê±´ì˜ ìœ ì‚¬ì‚¬ë¡€ë¥¼ í•œ ë²ˆì— ë³´ê³ 
    'ì¢…í•©ì  ì²˜ë¶„Â·ì¡°ì¹˜ ê¶Œê³ 'ë¥¼ ë§Œë“¤ì–´ ì¤Œ.
    """
    if client is None:
        raise RuntimeError("OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    cases_str = ""
    for i, c in enumerate(top_cases, 1):
        docs_str = ""
        for d in c["documents"]:
            docs_str += f"- ë¬¸ì„œID: {d.get('human_readable_id', d.get('id'))}, ì œëª©: {d.get('title')}\n"
        cases_str += f"\n[ì‚¬ë¡€ {i}]\në³¸ë¬¸: {c['text'][:800]}\në¬¸ì„œ ë©”íƒ€ë°ì´í„°:\n{docs_str or '(ì—†ìŒ)'}\n"

    prompt = f"""
ë‹¹ì‹ ì€ ê³µê³µê¸°ê´€ ë‚´ë¶€ê°ì‚¬Â·ì§•ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆì˜]
{query}

[ìƒìœ„ ìœ ì‚¬ì‚¬ë¡€ ëª¨ìŒ]
{cases_str}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•´ì„œ ì•„ë˜ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

1. ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œ ìœ í˜• (bullet í˜•ì‹)
2. ë²•ë ¹/ë‚´ê·œ ìœ„ë°˜ ì†Œì§€ê°€ í° í¬ì¸íŠ¸
3. ì¢…í•©ì ì¸ ì²˜ë¶„ ìˆ˜ì¤€ ê¶Œê³  (ì˜ˆ: ì£¼ì˜, ê²½ê³ , ê²½ê³ +ì œë„ê°œì„ , ì¤‘ì§•ê³„ ê²€í†  ë“±)
4. ì™œ ê·¸ ìˆ˜ì¤€ì´ ì ì ˆí•œì§€ì— ëŒ€í•œ ì„¤ëª…
5. í–¥í›„ ìœ ì‚¬ì‚¬ë¡€ ì˜ˆë°©ì„ ìœ„í•œ ì œë„ ê°œì„  ë˜ëŠ” ë‚´ë¶€í†µì œ ê°•í™” ë°©ì•ˆ (3~5ê°œ)
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ê°ì‚¬/ì¡°ì¹˜ ì „ë¬¸ê°€ì´ë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.3,
    )
    return resp.choices[0].message.content


# =========================================================
# 3. í‰ê°€ ì§€í‘œ ê³„ì‚° (Precision@k, Recall@k, MRR, HitRate)
# =========================================================
def precision_at_k(relevant: List[str], retrieved: List[str], k: int) -> float:
    retrieved_k = retrieved[:k]
    if not retrieved_k:
        return 0.0
    rel_set = set(relevant)
    hit = sum(1 for r in retrieved_k if r in rel_set)
    return hit / len(retrieved_k)


def recall_at_k(relevant: List[str], retrieved: List[str], k: int) -> float:
    if not relevant:
        return 0.0
    retrieved_k = retrieved[:k]
    rel_set = set(relevant)
    hit = sum(1 for r in retrieved_k if r in rel_set)
    return hit / len(rel_set)


def mrr_at_k(relevant: List[str], retrieved: List[str], k: int) -> float:
    rel_set = set(relevant)
    for i, r in enumerate(retrieved[:k]):
        if r in rel_set:
            return 1.0 / (i + 1)
    return 0.0


def hit_rate_at_k(relevant: List[str], retrieved: List[str], k: int) -> float:
    rel_set = set(relevant)
    return float(any(r in rel_set for r in retrieved[:k]))


# =========================================================
# 4. Streamlit UI
# =========================================================
st.title("ğŸ“˜ ê°ì‚¬ ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰ Â· ê·¼ê±° ê·¸ë˜í”„ Â· ì²˜ë¶„ ì¶”ì²œ")

st.markdown(
    """
ì´ ì•±ì€ **GraphRAG ì¸ë±ì‹± ê²°ê³¼(output í´ë”)** ë¥¼ ê¸°ë°˜ìœ¼ë¡œ

1. ğŸ” ìœ ì‚¬ ê°ì‚¬ì‚¬ë¡€ ê²€ìƒ‰  
2. ğŸ§  ì‚¬ë¡€ ë¶„ì„ + ì„¤ëª…ê°€ëŠ¥í•œ ì²˜ë¶„ ì¶”ì²œ  
3. ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ(Precision@k, Recall@k, MRR, HitRate)  

ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
"""
)

if client is None:
    st.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ë°ì´í„° ë¡œë”© (í•œ ë²ˆë§Œ)
try:
    data = load_graph_index(OUTPUT_DIR)
except Exception as e:
    st.error(f"GraphRAG output ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()

tab_search, tab_overall, tab_eval = st.tabs(
    ["ğŸ” ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰ & ë‹¨ì¼ ì‚¬ë¡€ ë¶„ì„", "ğŸ§  ì¢…í•© ì²˜ë¶„ ì¶”ì²œ", "ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"]
)

# ---------------------------------------------
# 4-1. ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰ & ë‹¨ì¼ ì‚¬ë¡€ ë¶„ì„
# ---------------------------------------------
with tab_search:
    st.subheader("ğŸ” ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰")

    query = st.text_input(
        "ê°ì‚¬ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”. (ì˜ˆ: ì˜ˆì‚° ë¶€ì ì • ì§‘í–‰, ìš©ì—­ê³„ì•½ ìœ„ë²• ë“±)",
        key="query_input",
    )

    top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (Top-K)", min_value=3, max_value=30, value=10, step=1)

    if st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary"):
        if not query.strip():
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("ì„ë² ë”© ê²€ìƒ‰ ì¤‘..."):
                try:
                    top_df, scores = semantic_search(query, data, top_k=top_k)
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                else:
                    st.session_state["last_query"] = query
                    st.session_state["last_results"] = top_df
                    st.success(f"{len(top_df)}ê±´ì˜ ìœ ì‚¬ì‚¬ë¡€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
    last_results: pd.DataFrame = st.session_state.get("last_results")
    last_query: str = st.session_state.get("last_query", "")

    if last_results is not None:
        st.markdown("---")
        st.markdown("### ğŸ” ê²€ìƒ‰ ê²°ê³¼ (ìœ ì‚¬ë„ ìˆœ)")

        for idx, row in last_results.iterrows():
            case_info = build_case_summary(row, data)
            sim = row["similarity"]

            # ì¹´ë“œ ìŠ¤íƒ€ì¼ ì¶œë ¥
            with st.container(border=True):
                doc_ids_str = ", ".join(
                    d.get("human_readable_id", str(d.get("id"))) for d in case_info["documents"]
                ) or "ë¬¸ì„œ ì •ë³´ ì—†ìŒ"

                st.markdown(
                    f"**ì‚¬ë¡€ ID:** `{case_info['text_id']}` &nbsp;&nbsp; "
                    f"**ì—°ê´€ ë¬¸ì„œ:** {doc_ids_str} &nbsp;&nbsp; "
                    f"**ìœ ì‚¬ë„:** `{sim:.3f}`"
                )
                st.markdown(
                    f"<div style='font-size:0.9rem;'>{case_info['text'][:300]}...</div>",
                    unsafe_allow_html=True,
                )

                # ìƒì„¸ ë¶„ì„(LLM í˜¸ì¶œ)
                with st.expander("ğŸ§  ì´ ì‚¬ë¡€ ê¸°ë°˜ ìƒì„¸ ë¶„ì„ & ì²˜ë¶„ ì¶”ì²œ ë³´ê¸°"):
                    if st.button("ì´ ì‚¬ë¡€ ë¶„ì„ ì‹¤í–‰", key=f"analyze_{case_info['text_id']}"):
                        with st.spinner("LLMì´ ì‚¬ë¡€ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                            try:
                                analysis = llm_analyze_case(last_query, case_info)
                                st.markdown(analysis)
                            except Exception as e:
                                st.error(f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

# ---------------------------------------------
# 4-2. ì—¬ëŸ¬ ì‚¬ë¡€ ê¸°ë°˜ ì¢…í•© ì²˜ë¶„ ì¶”ì²œ
# ---------------------------------------------
with tab_overall:
    st.subheader("ğŸ§  ìƒìœ„ ìœ ì‚¬ì‚¬ë¡€ ê¸°ë°˜ ì¢…í•© ì²˜ë¶„ ì¶”ì²œ")

    last_results: pd.DataFrame = st.session_state.get("last_results")
    last_query: str = st.session_state.get("last_query", "")

    if last_results is None:
        st.info("ë¨¼ì € [ğŸ” ìœ ì‚¬ì‚¬ë¡€ ê²€ìƒ‰] íƒ­ì—ì„œ ê²€ìƒ‰ì„ í•œ ë²ˆ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    else:
        st.markdown(f"**í˜„ì¬ ê¸°ì¤€ ì§ˆì˜:** `{last_query}`")
        num_cases = st.slider(
            "ì¢…í•© ë¶„ì„ì— ì‚¬ìš©í•  ìƒìœ„ ì‚¬ë¡€ ê°œìˆ˜",
            min_value=3,
            max_value=min(20, len(last_results)),
            value=min(5, len(last_results)),
            step=1,
        )

        if st.button("ì¢…í•© ì²˜ë¶„ ì¶”ì²œ ìƒì„±", type="primary"):
            with st.spinner("ìƒìœ„ ì‚¬ë¡€ë“¤ì„ ì¢…í•©í•´ ì²˜ë¶„ ì¶”ì²œì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    top_cases_info = [
                        build_case_summary(row, data)
                        for _, row in last_results.head(num_cases).iterrows()
                    ]
                    summary = llm_recommend_overall(last_query, top_cases_info)
                    st.markdown(summary)
                except Exception as e:
                    st.error(f"ì¢…í•© ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

# ---------------------------------------------
# 4-3. ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ íƒ­
# ---------------------------------------------
with tab_eval:
    st.subheader("ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ (Precision@k / Recall@k / MRR / HitRate)")

    st.markdown(
        """
**ì‚¬ìš© ë°©ë²•**

1. ì•„ë˜ í˜•ì‹ì˜ JSON íŒŒì¼ì„ ì—…ë¡œë“œ í•©ë‹ˆë‹¤.

```json
[
  {
    "query": "ì˜ˆì‚° ë¶€ì ì • ì§‘í–‰",
    "relevant_ids": ["DOC-001", "DOC-005"],
    "k": 10
  },
  {
    "query": "ìš©ì—­ ê³„ì•½ ì§€ì—°",
    "relevant_ids": ["DOC-010"],
    "k": 10
  }
]
""")